\chapter{并行化算法}
\label{chp:4}

算法并行化的第一步通常是设计一个符合待处理问题及处理器架构特点的并行方案，然后根据该方案确定相应的编程模型，最后分析并行程序的性能，有针对性地做具体的优化。
并行程序的性能优化和普通串行程序有许多共通之处。具体来说主要分为两个大类：针对内存使用的优化和针对for循环的改进。

由于日益增长的处理器速度和发展相对较缓的内存速度之间逐渐拉开了差距，内存通常成为程序蓄能的瓶颈。现代处理器通常采用分层内存体系，高带宽，低延迟（lower latency），低能耗的高速缓存对提升数据局部性(data locality)至关重要。针对缓存优化的方法有很多，例如对齐数据（data alignment）使得缓存的加载更有效率，依据缓存的大小处理相应的数据块（cache blocking），预读取（prefetching）提前将需要处理的数据加载到缓存中，或者通过流式存储技术（streaming store）将非时间相关（nontemporal）的计算结果"绕过"缓存直接写入主内存从而避免"污染"缓存其他数据等。和数据代码缓存一样，页表缓存（Translation Lookaside Buffer）也是影响性能的一个因素。页表缓存存储了一部分标签页表条目，用于改进从虚拟内存地址到物理地址的转译速度，对于特定问题，通过使用不同尺寸的页面大小，重构数据可以避免页表缓存中的热点。

针对for循环优化的一般技术包括循环展开（loop unrolling）,分块循环（loop tiling）, 循环互换（loop interchange），循环合并（loop fusion），循环偏移（loop skewing），循环剥离（loop peeling）等。这些改进很多时候也是为了更好地使用内存或者为进一步并行化做准备。
目前绝对意义上的串行程序在实际中已不多见，因为在编译过程中编译器会或多或少引入不同程度的并行化处理。然而由于编译器优化的前提是基于代码的正确性，串行程序的逻辑遵循一个绝对的代码执行顺序，因而其编写和调试难度远低于并行程序。并行程序的运行由于同时驱动不同的逻辑运算单元，容易出现数据的竞态条件（race condition）等导致最终结果不确定的因素。本章将着重描述几种针对本课题不同的并行方案，及其具体实现。

\section{并行性分析}
\label{sec:parallelism}
并行的目的就是在充分利用单机资源的基础上，将计算扩展到多个机器，在可接受的时间范围内处理规模更大，复杂度更高的问题。
在单个机器上，原则上我们需要发掘串行算法中的并行机会，将基本的并发性操作表述为任务（task）的形式提交给调度器（scheduler），再由调度器将其分发给不同的线程执行。在MIC这样的众核处理器上，足够的任务级并行度对于充分利用片上丰富的线程资源非常重要。由于时间的限制我们选择不重新实现一个新的调度算法，而是使用现有的工具，如OpenMP，Cilk+或者Threading Building Blocks(TBB)。
在多个节点上，我们采用消息传递模式如MPI来进行通信，目标是尽量降低通信量的同时，将计算和通信重叠起来，提高整体效率。

算法\ref{alg:bserror}描述了欧式期权对冲策略误差控制的串行算法。本章的核心目的就是论述基于该算法的并行化方案。
分析该算法可知蒙特卡洛模拟的结果依赖每次模拟求得的误差$error$，而求$error$的过程除了两项非随机量，其余均与$PX$相关。数组$PX$表示的是在离散时间点的期权价格，取值和一个随机布朗运动的各个状态相关联。该布朗运动可视作一个马尔科夫过程，前后两项相差一个符合高斯分布的随机数。由于其每两项之间存在数据依赖性，使得其并行度十分有限。因此问题的关键在于如何有效地打破这种数据之间的依赖性。本文所讨论的数据依赖性本质上源自于N个独立高斯分布的随机数组成的数组$NRV[N]$。对于一个给定的N和从$BM[N]$中选取的任何一段连续的子集$bm[n]$，如果可以并行地处理这些分段子集，我们就可以极大地提高计算效率，实现本节最初提出的目标。

计算某个子集的条件是已知其初始状态和该子集所对应$NRV[N]$中的子集。而该子集的初始状态可表示为其之前所有状态对应的正态分布随机数之和乘以一个常量。
因此有两种并行思路。第一种是将整个长度为N的数组分为连续的子集，按顺序每次所有的计算资源并发地参与计算其中一段子集，计算完成该段子集之后再计算紧邻的下一段。如此，计算下段子集的时候，它的初态就是上一段的终态加上一个随机数，而上段的终态是已知的。依据这种方案，基本的并行任务（task）是已知某段子集对应的所有的随机数，计算其中某个布朗运动状态，其对应的离散时间点的期权价格，以及最终的误差。

第二种思路是将我们之前所讨论的某段子集的处理直接作为基本的并行任务分配给线程计算。这样的话子集的初始状态是未知的，因为不能假设某个线程先后处理的是连续的子集。在这种情况下，需要根据之前生成的随机数来推断某段子集的初始状态。这就无法回避一个重要的问题，随机数发生器。

在目前的计算机应用里，几乎所有用到的随机数发生器都只能产生伪随机数。其原理通常是根据某种算法计算出一连串在实际应用中可认为相互独立的数。当该算法的结果空间足够大的时候，生成的伪随机数几乎没有可能重复，从而保证了算法的有效性。在该课题中，我们使用的是基于梅森旋转算法（Mersenne twister）的高斯分布随机数发生器，它有$2^{19937}-1$的非常长的周期。该随机数发生器的并行版本，在于生成不同的随机流（stream），不同的流使用不同的“种子（seed）”在不同的结果子空间中计算随机数，相互之间保持独立。

在我们的第一种并行思路中，我们只需要生成一条随机流，每次计算某段子集的时候空闲的线程利用随机数发生器生成下一个紧邻子集对应的随机数。所有随机数由全部线程共享，从而解除了数据的依赖性。
在第二种并行思路中，由于某段子集作为一个基本的任务分配给线程，不同线程间异步地对不同段子集进行操作，因此无法共享单条随机数组。在这种情况下，每个线程保有一条私有的随机数流，根据自己计算的进度按需生成相应的随机数。这种情况下对于数据依赖性的处理则在于随机数的“种子”。对于不同的流使用不同的种子固然是可以得到互相独立的随机数组。但如果使用相同的种子则会生成完全一样的随机数组。由于不同的线程虽然处理的子集不同，但都是针对同一个给定$N$的离散价格误差的计算，$N$个随机数理应一致。通过不同的随机流，不同的线程独立生成随机数，避免了相互之间的通信，从而解除了数据的依赖性。

根据以上提供的两种思路，我们利用Intel OpenMP实现了单机上的并行版本。由于该算法的最终目的是寻找一个最优的$N$以满足给定的可容忍概率。每一次迭代都需要$M$次蒙特卡洛模拟计算一个新的$N$值。在实际中，只有相对较大的$M$值才能得到令人信服的结果。因此对于把单机版本扩展到多个节点或机器，我们选择在$M$这个维度上并行。基本思路是基于Master-Slave模型由一个主机（boss）收集由工作机（worker）发送过来的结果，分析整理之后决定下一次计算的$N$\footnote{关于$N$的最优化搜索具体讨论见第\ref{sec:searchN}节}值并发送给工作机。工作机在等待指令的同时自行选取一个更小的$N$值继续计算。接受到主机指令之后假如自行计算的$N$值和主机指令相同，则继续完成。否则舍弃计算结果重新开始一个新的$N$值的计算。为了更好地重叠计算和通信，我们采用非阻塞通信模式，如此，工作机可以充分发挥计算效能，避免空隙或等待。具体的实现细节参见第\ref{sec:mpi}节。

\section{单机并行算法}
\label{sec:monoparallel}
根据上节\ref{sec:parallelism}的分析，我们分别在算法\label{alg:omp1}，\label{alg:omp2_1}和\label{alg:omp2_2}中给出两种单机并行思路的具体实现细节。在实际中，我们采用Intel MKL（Math Kernel Library）提供的高斯分布随机数发生器。该发生器基于梅森旋转算法。每次可生成单个或多个随机数。
\subsection{方案一}
该方案将整段N分为相邻的子集，子集内部连续，所有线程依次对每一段子集进行并行处理。
\begin{algorithm}
  \caption{基于多线程（multithreading）和矢量化（vectorization）的单机并行算法一(单次蒙特卡洛模拟)}
  \label{alg:omp1}
  \begin{algorithmic}[1]
    \Require $X_0, \sigma, K, T, \epsilon, M, N$ (参见算法\ref{alg:bserror})
    \Require $N_{cache}$ \Comment{依序并行处理的子集大小}
    \Ensure $count$
    \Procedure{$MONO_1$}{M, N}
    \State $error \gets 0$
    \State $count \gets 0$
    \State $NRV[N_{cache}] \gets vdRngGaussian(N_{cache})$ \Comment{一次生成$N_{cache}$个符合高斯分布的随机数，赋给线程共享的NRV数组}
    \State $BM[N_{cache}]$ \Comment{线程共享的BM数组}
    \State $PX[N_{cache}+1]$ \Comment{线程共享的PX数组}
    \State $PX[0] \gets 0$
    \State $\delta t \gets T/N$
    \State $BM[0] \gets \sqrt{\delta t} \times NRV[0]$
    \For{$k=1:(int)N/N_{cache}$}
    \State \textbf{Start parallel for region} \Comment{调度器将不同的i分配给不同线程}
    \For{$i=1:N_{cache}$}
    \State $BM[i] = BM[0] + \sqrt{\delta t}\times reduce\_add(NRV[1:i])$ \Comment{闭区间SIMD reduction操作}
    \State $PX[i+1] = X_0 \times \exp(-0.5 \sigma^2 \times (kN_{cache}+i+1) \times \delta t + \sigma \times BM[i])$
    \EndFor
    \State \textbf{End parallel for region}
    \State $PX[1] = X_0 \times \exp(-0.5 \sigma^2 \times (kN_{cache}+1) \times \delta t + \sigma \times BM[0])$
    \State \textbf{Start parallel region reduction(+:error) nowait}
    \For{$i=0:N_{cache}$} 
    \State $j \gets kN_{cache}+1$
    \State $T_j \gets {j \times T}/{N}$
    \State $Upper = (\log(PX[i]/K)+0.5\times \sigma^2 \times (T-T_j))/(\sigma \times \sqrt{T-T_j})$
    \State $error \gets error - \frac{1}{\sqrt{2\pi}}\times (PX[i+1]-PX[i])\times \int_{-\infty}^{Upper1}e^{-\frac{t^2}{2}}dt$
    \EndFor
    \State \textbf{End parallel region reduction(+:error) nowait}
    \State $NRV[N_{cache}] \gets vdRngGaussian(N_{cache})$ 
    \State $BM[0] = BM[N_{cache}-1] + \sqrt{\delta t}\times NRV[0]$
    \State $PX[0] \gets PX[Ncache]$
    \EndFor
    \State $以类似方式处理剩余的N\%N_{cache}次循环$
    \State $error = error + K/\sqrt{2\pi} \cdot \int_{-\infty}^{Upper2}e^{-\frac{t^2}{2}}dt -X0/\sqrt{2\pi}\cdot \int_{-\infty}^{Upper1}e^{-\frac{t^2}{2}}dt$
    \If{$PX[end]>K$}
    \State $error \gets error + (PX[end]-K)$
    \EndIf
    \If{$abs(err) < \epsilon$}
    \State $count \gets 1$
    \EndIf
    
    \State return $count$
    
    \EndProcedure
  \end{algorithmic}
\end{algorithm}
方案一中，每次模拟中$N$维的向量（$NRV, BM, PX$）被分成大小为$N_{cache}$（$PX$维度高一）的紧邻相连的子集。每个子集被分配给全体线程共同计算，子集间仍保持一个绝对的顺序。多线程环境下，$NRV$,$BM$,$PX$由全体CPU核心共享。因此该并行算法性能提升的关键在于调节$N_{cache}$的大小使得共享数据可以填充缓存。数据在缓存内，存取延迟小带宽高，和CPU计算速度匹配。如果共享数据不够大，没有充分利用缓存空间，所需的循环次数相对较多，则相应的线程调度开销（scheduling overheads）就会偏大。如果共享数据过大，缓存无法完整包含，则会导致与主内存频繁的数据交换，亦会降低程序的性能。

在MIC架构下，每个核心拥有两级缓存。$L2$缓存完整包含$L1$缓存的内容。单个核心拥有的$L2$缓存的大小是$512K$，这些缓存通过片上环形网络相连。同时与环形网络相连的还有一个标签目录系统（tag directories），它的存在使得整个L2缓存保持一致（coherent）。因此当数据存在任何核心的$L2$缓存下，都可以相对较快地被其他核心调用。$L2$缓存的全部容量是$512K\times 61 = 31M$，最优的共享数据尺寸应和该大小相符。关于$N_{cache}$在CPU和MIC上的调优，我们将在下章\ref{chap:exp}中详细论述。

\subsection{方案二}
该方案将整段N分为相邻的子集，每段子集内部连续，不同子集被分配给不同线程并发地处理。
\begin{algorithm}
  \caption{基于多线程（multithreading）和矢量化（vectorization）的单机并行算法二(单次蒙特卡洛模拟)，第一部分}
  \label{alg:omp2_1}
  \begin{algorithmic}[1]
    \Require $X_0, \sigma, K, T, \epsilon, M, N$ (参见算法\ref{alg:bserror})
    \Require $N_{vec}$ \Comment{分配给某线程的某子集的大小}
    \Ensure $count$

    \Procedure{$MONO_2$}{$M, N$}
    \State $error \gets 0$
    \State $count \gets 0$
    \State \textbf{Start parallel region} 
    \State $NRV[N_{vec}] \gets vdRngGaussian(N_{vec})$ \Comment{线程私有的NRV数组} 
    \State $BM[N_{vec}]$ \Comment{线程私有的BM数组}
    \State $PX[N_{vec}+1]$ \Comment{线程私有的PX数组}
    \State $errloc \gets 0$ \Comment{线程私有误差记录}
    \State $BMlast \gets 0$ \Comment{线程私有，记录前一次计算的BM（布朗运动）的终态}
    \State $PXlast \gets 0$ \Comment{线程私有，记录前一次计算的PX（离散价格）的终态}
    \State $lastk \gets -1$ \Comment{线程私有，记录前一次处理的子集的序号}
    \State $stream(seed)$ \Comment{线程私有，以seed为种子随机数发生流}
    \State \textbf{Start parallel for region} \Comment{调度器将不同k分配给不同线程}
    \For{$k=0:(int)N/N_{vec}$}
    \State $tmp \gets 0$
    \For{$i=0:k-lastk-1$}
    \State $NRV[N_{vec}] \gets vdRngGaussian(N_{vec})$ \Comment{一次生成$N_{vec}$个符合高斯分布的随机数}
    \State $tmp \gets tmp + reduce\_add(NRV[0:N_{vec}-1])$ \Comment{闭区间SIMD reduction操作}
    \EndFor
    \State $BMlast = BMlast + \sqrt{\delta t}\times tmp$
    \State $PXlast = X_0 \times \exp(-0.5 \sigma^2 \times (kN_{vec}) \times \delta t + \sigma \times BMlast)$
    \State $NRV[N_{vec}] \gets vdRngGaussian(N_{vec})$ \Comment{一次生成$N_{vec}$个符合高斯分布的随机数}
    \State $BM[0] \gets BMlast + \sqrt{\delta t}\times NRV[0]$
    \State $PX[0] \gets PXlast$

    \For{$i=1:N_{vec}$}
    \State $BM[i] = BM[0] + reduce\_add(NRV[1:i])$ \Comment{SIMD reduction操作}
    \State $PX[i+1] = X_0 \times \exp(-0.5 \sigma^2 \times (kN_{vec}+i+1) \times \delta t + \sigma \times BM[i])$
    \EndFor
    \State $PX[1] = X_0 \times \exp(-0.5 \sigma^2 \times (kN_{vec}+1) \times \delta t + \sigma \times BM[0])$

    \For{$i=0:N_{vec}$}
    \State $j \gets k\times N_{vec}+i$
    \State $T_j \gets j\times T/N$
    \State $Upper = (\log(PX[i]/K)+0.5\times \sigma^2 \times (T-T_j))/(\sigma \times \sqrt{T-T_j})$
    \State $error \gets error - \frac{1}{\sqrt{2\pi}}\times (PX[i+1]-PX[i])\times \int_{-\infty}^{Upper1}e^{-\frac{t^2}{2}}dt$
    \EndFor
    
    \State $BMlast \gets BM[N_{vec}-1]$
    \State $PXlast \gets PX[N_{vec}]$
    \State $lastk \gets k$

    \EndFor
    \State \textbf{End parallel for region}

    \If {$lastk=nCal \land N\%Nvec=0$}
    \If {$PX[Nvec]>K$}
    \State $errloc \gets errloc + PX[N_{vec}] - K$ 
    \EndIf
    \EndIf

    \algstore{break}
    \end{algorithmic}
  \end{algorithm}

方案一中，每次模拟中$N$维的向量（$NRV, BM, PX$）被分成大小为$N_{vec}$（$PX$维度高一）的子集。每个子集被分配给一个线程单独处理，以私有的方式存储。另外，每个线程也保有一条私有的随机数发生流。因此该并行算法性能提升的关键在于调节$N_{vec}$的大小使得数据可以填充缓存线程所在核心的缓存。和算法\label{alg:omp1}一样，数据在缓存内，存取延迟小带宽高，和CPU计算速度匹配。如果$N_{vec}$太小，没有充分利用缓存空间，所需的循环次数相对较多，则相应的线程调度开销（scheduling overheads）就会偏大。如果$N_{vec}$过大，单个核心的缓存无法完整包含，则会导致与主内存频繁的数据交换，亦会降低程序的性能。

在MIC架构下，每个核心拥有$L1$缓存包括$32K$的数据缓存和$32K$的代码缓存，以及$512K$的$L2$缓存，由代码和数据共同分享。最优的$N_{vec}$应使各线程私有数据尺寸和$L2$缓存大小相符。关于$N_{vec}$在CPU和MIC上的调优，我们将在下章\ref{chap:exp}中详细论述。


\begin{algorithm}
  \caption{基于多线程（multithreading）和矢量化（vectorization）的单机并行算法二(单次蒙特卡洛模拟)，第二部分}
  \label{alg:omp2_2}
  \begin{algorithmic}[1]

    \algrestore{break}
    \State $以类似方式处理剩余的N\%N_{vec}次循环并相应更新errloc如果PX[N\%Nvec]>K$
    
    \State \textbf{Start single} \Comment{单线程操作}
    \State $errloc = errloc + K/\sqrt{2\pi} \cdot \int_{-\infty}^{Upper2}e^{-\frac{t^2}{2}}dt$
    \State \textbf{End single}

    \State \textbf{Start single} \Comment{单线程操作}
    \State $errloc = errloc - X0/\sqrt{2\pi}\cdot \int_{-\infty}^{Upper1}e^{-\frac{t^2}{2}}dt$
    \State \textbf{End single}

    
    \State \textbf{Start atomic operation} \Comment{原子操作}
    \State $error \gets error + errloc$
    \State \textbf{End atomic operation}

    \State \textbf{End Parallel region}    

    \If{$abs(err) < \epsilon$}
    \State $count \gets 1$
    \EndIf
    
    \State return $count$

    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\section{矢量化积分运算}
\label{sec:vecintegral}
在算法\ref{alg:omp1}，\ref{alg:omp2_1}和\ref{alg:omp2_2}中大量使用了如下一个积分：

\begin{equation}
f(x)=\int_{-\infty}^{x}e^{-\frac{t^2}{2}}dt
\end{equation}

这实际上是一个高斯分布概率密度的积分。毋庸置疑，计算这个积分的效率将直接影响整个程序的性能和精度。结合我们在上节中叙述的并行算法，需要计算这个积分的通常是单个线程。因此我们这里介绍一个矢量化（vectorized）的并行积分计算。
利用辛普森积分法（Simpson's rule）
\begin{equation}
\label{eq:simpson}
\int_{a}^{b}f(x)dx\approx \frac{h}{3}\left[f(x_0)+2\sum\limits_{j=1}^{n/2-1}f(x_{2j})+4\sum\limits_{j=1}^{n/2}f(x_{2j-1})+f(x_n)\right]
\end{equation}
又已知高斯积分结果有
\begin{equation}
\label{eq:gauss}
f(x)=\int_{-\infty}^{x}e^{-\frac{t^2}{2}}dt=0.5\times \sqrt{2\pi} + \int_{0}^{x}e^{-\frac{t^2}{2}}dt
\end{equation}

由公式\ref{eq:simpson}和\ref{eq:gauss}，我们可得以下串行算法\ref{alg:simpson_ser}。

\begin{algorithm}
  \caption{基于Simpson公式的高斯积分串行算法}
  \label{alg:simpson_ser}
  \begin{algorithmic}[1]
    \Require $N$ \Comment{积分区间细分程度}
    \Ensure $sum$
    \Procedure{$NormalIntegral$}{$b$}
    \State $a \gets 0$
    \State $sum \gets 0$
    \State $h \gets \frac{b-a}{NN}$
    \State $sum \gets sum + exp(-\frac{a^2}{2}) + 4exp(-\frac{(a+h)^2}{2} + exp(-\frac{b^2}{2}))$
    \For{$i=1:N/2$}
    \State $s \gets a+2\times i \times h$
    \State $sum \gets sum + 2exp(-\frac{s^2}{2})$
    \State $s \gets s + h$
    \State $sum \gets sum + 4exp(-\frac{s^2}{2})$
    \EndFor
    \State $sum \gets 0.5\times \sqrt{2\pi} + h\times sum/3$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

得益于级数天然的矢量形式，我们利用包装汇编代码的内部函数（Intrinsic function）将算法\ref{alg:simpson_ser}改写为直观的矢量形式，以期获得更好的性能。算法表述如\ref{alg:simpson_par}。

\begin{algorithm}
  \caption{基于Simpson公式的高斯积分矢量化算法}
  \label{alg:simpson_par}
  \begin{algorithmic}[1]
    \Require $N$ \Comment{积分区间细分程度}
    \Require $vecsize$ \Comment{矢量位长}
    \Ensure $sum$
    \Procedure{$vNormalIntegral$}{$b$}
    \State $vec_{cf0}, vec_{cf1}, vec_{cf2}, vec_{s}, vec_{stp}, vec_{exp}$
    \State $a \gets 0$
    \State $sum \gets 0$
    \State $h \gets \frac{b-a}{N}$
    \State $sum \gets sum + exp(-\frac{a^2}{2}) + 4exp(-\frac{(a+h)^2}{2} + exp(-\frac{b^2}{2}))$
    \State $vec_{cf0} \gets set1(a)$ \Comment{$矢量寄存器置a$}
    \State $vec_{cf1} \gets set1(2\times h)$ \Comment{$矢量寄存器置2\times h$}
    \State $vec_{cf2} \gets set1(-0.5)$ \Comment{$矢量寄存器置-0.5$}

    \State $vec_{s} \gets set(8,7,6,5,4,3,2,1)$ \Comment{$将矢量寄存器从高到低置8到1$}
    \State $vec_{s} \gets mul(vec_s, vec_{cf1})$ \Comment{$矢量对应项相乘： 16h, 14h, ..., 2h$}
    \State $vec_{s} \gets add(vec_{cf0}, vec_s)$ \Comment{$矢量对应项相加：a+16h, ..., a+2h$}
    
    \State $vec_{stp} \gets set1(2h\times vecsize - h)$ \Comment{$循环中vec_s的增量步长$}
    \State $vec_{cf0} \gets set1(h)$ \Comment{$矢量寄存器置h$}

    \For{$i\gets0:(N/2-1)/vecsize$}
    \State $vec_{exp} \gets mul(vec_s, vec_s)$
    \State $vec_{exp} \gets mul(vec_{exp}, vec_{cf2})$
    \State $vec_{cf1} \gets exp(vec_{exp})$
    \State $sum \gets sum + 2\times reduce\_add(vec_{cf1})$
    
    \State $vec_{s} \gets add(vec_{s}, vec_{cf0})$ \Comment{$s=s+h$}
    \State $vec_{exp} \gets mul(vec_{s}, vec_{s})$ 
    \State $vec_{exp} \gets mul(vec_{exp}, vec_{cf2})$ 
    \State $vec_{cf1} \gets exp(vec_{exp})$
    \State $sum \gets sum + 4\times reduce\_add(vec_{cf1})$

    \State $vec_{s} \gets add(vec_{s}, vec_{stp})$ \Comment{$s=s+2h\times vecsize - h$}
    \EndFor
    \State $sum \gets 0.5\times \sqrt{2\pi} + h\times sum/3$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

这里积分区间的细分程度$N$需要根据算法\ref{alg:bserror}中每次蒙特卡洛模拟选取的$N$相匹配。蒙特卡洛模拟中选取的$N$越大，则需要将高斯积分区间分得越细来满足精度要求。
经过矢量化的高斯积分计算函数在实际中对比未经矢量化的版本，在CPU和MIC上的性能均有显著提高。具体性能分析请参见第\ref{chap:exp}



\section{N的最优化搜索}
\label{sec:searchN}
算法\ref{alg:bserror}的最终目的是寻找一个最小的$N$值以满足离散策略误差可容忍下限。本文一个核心贡献即是第\ref{chp:3}章中的一个原创结果，该结果给出了一个优于以往任何文献发表的对于$N$值的估计。该估计给出了与误差同阶的上界，将问题转化在有限闭区间中最优解的搜索，使得二分法的应用成为可能。

二分法广泛应用于各个科学工程领域，它将搜索的复杂度从$O(N)$降为$O(log(N))$，这和我们给出的上界是分不开的。

从上界$N_0$开始，在获得一个可接受的$N$值后，我们可以继续寻找满足条件的更小的$N$值，直至算法收敛。二分的过程其实就是不断修正搜索的上界和下界的过程。当上界和下界的差值小于我们预设的门限，则判定算法收敛，停止搜索。算法\ref{alg:searchN}给出了当前语境下二分法的具体实现。
\begin{algorithm}
	\caption{最优$N$值的搜索算法}
	\label{alg:searchN}
	\begin{algorithmic}[1]
	  \Require $M$ \Comment{蒙特卡洛模拟的次数，$M$越大模拟结果可信度越高}
          \Require $Prob$ \Comment{离散策略误差可接受概率下限}
          \Ensure $n$ 
	  \Procedure{NBSECT}{$N_0$, $\lambda$} \Comment{$N_0$是初始抽样次数，$\lambda$是收敛判定门限}
          \State $lastvalid \gets -1$ \Comment{$最近一次满足条件的N值$}
	  \State $nL \gets 0$ \Comment{设置$n$值的初始下界} 
	  \State $nU \gets N_0$ \Comment{设置$n$值的初始上界}
	  \State $n \gets \frac{nL + nU}{2}$
	  \While {$|nU - nL| > \lambda$} 
	  \If {$BSERROR(M,n) > Prob$} \Comment{修改上界，搜索更小的N}
          \State $lastvalid \gets n$
	  \State $nU \gets n$
	  \State $n \gets \frac{nU + nL}{2}$		
	  \Else  \Comment{修改下界，搜索更大的N}
	  \State $nL \gets n$
	  \State $n \gets \frac{nU + nL}{2}$
	  \EndIf
	  \EndWhile
          \If{$lastvalid!=-1$}
	  \State return $n$
          \Else 
          \State $未搜索到满足条件的N值$ 
          \EndIf 
	  \EndProcedure
	\end{algorithmic}
\end{algorithm}
在算法 \ref{alg:searchN} 中我们设置的收敛条件为$|nU - nL| \le \lambda$，$lambda$可根据计算资源，实际应用中要求的精度进行调整。由于蒙特卡洛模拟本身有其随机性，当$M$值较小时在$N$的收敛过程中会出现波动，可能导致无法每次试验无法收敛到单一的$N$值上。因此在实际中我们需要取一个相对较大的$M$来保证蒙特卡洛模拟的结果是可信的。当$M$增大到一定程度每次$N$收敛的值趋于稳定，我们可认为该$M$值是有效的。当然，更大的$M$值从数值上带来更好的近似程度，然而却需要更多的计算资源。关于如何选取一个合适的$M$详见下章。

\section{多机并行}
\label{sec:mpi}
如\ref{sec:parallelism}中所述，多机并行采用消息传递模型，实现上使用Intel MPI。算法上我们采用Master-Slave模型，在一个或多个节点上由CPU来扮演boss的角色，而其余的机器则完成worker的工作。具体来说，从初始的$N$值开始，多机在$M$（蒙特卡洛模拟次数）这个维度上并行。每一次除掉boss，其余worker平分$M$并发地计算一个给定$N$值下的蒙特卡洛模拟，而boss则分担$M$除以worker数量无法除尽的余数。当worker的数量不是非常多的时候，boss分担的任务是十分有限的，因为他的主要任务是接受来自其他worker的结果，并作出判断决定下次迭代的$N$值。他将从不同worker发来的$M$和$count$（$M$次中满足条件的次数）加总之后利用算法\ref{alg:searchN}得到一个新的$N$值，并随后将这个$N$值分发给各个worker。假如搜索算法已满足收敛条件，则boss将停止工作的讯息发给各个worker使后者停止正在行进的工作。作为worker来说，如果boss分配$N$值对应的$M$和$count$已经计算完毕，他将自行根据之前的上下界计算一个更小的$N$值。计算时每间隔一段时间检查是否收到boss的讯息。收到boss讯息之后，停止计算，对比自己计算的$N$是否和boss发来的一致，如果一致，则下一次迭代减少相应的计算次数，如果不一致，则舍弃已获得的计算结果，重新开始计算一个新的$N$值，同时更新上下界。在实际中$M$值一般较大，所有的worker可能由于通信，网络等各种因素随机地出现延迟，但本文描述的这种设计可以避免预先完成的worker浪费机时等待指令。与此同时，我们采用非阻塞通信来沟通boss和worker，因此woker可以以极高的效率投入计算当中。
预先完成的worker自行计算一个更小的$N$的策略在实际中往往获得非常好的效果。具体分析请见第\ref{chap:exp}章。
算法\ref{alg:mpi}描述的是多机并行的实现细节。
\begin{algorithm}
	\caption{基于MPI的多机并行算法，boss部分}
	\label{alg:mpi}
	\begin{algorithmic}[1]
	  \Require $M$ \Comment{蒙特卡洛模拟的次数，$M$越大模拟结果可信度越高}
          \Require $N_0$ \Comment{由理论给出的离散抽样次数上界}
          \Require $\lambda$ \Comment{收敛判定门限}
          \Require $Prob$ \Comment{离散策略误差可接受概率下限}
          \Require $mpiWorldSize$ \Comment{参与计算的进程数}
          \Require $myRank$ \Comment{进程id}
          \Ensure $n$ \Comment{满足条件的最小的N}
	  \Procedure{MULTI}{$N_0$, $\lambda$} \Comment{$N_0$是初始抽样次数，$\lambda$是收敛判定门限}
          \State $Nloc, Nnew, Nup, Ndown$ \Comment{当前的N，下一次的N，N搜索的上界，N搜索的下界}
          \If{myRank==0} \Comment{第一个进程为boss}
          \State $Nup \gets N_0$
          \State $Ndown \gets 0$
          \State $Nloc \gets \frac{Nup+Ndown}{2}$
          \State $lastvalid \gets -1$ \Comment{最近一次满足条件的N值}
          \State $stop \gets 0$
          \While {$!stop$}
          \State $sumM \gets M\%(mpiWorldSize-1)$
          \State $sumCount \gets 0$
          \For{src=1:mpiWorldSize}
          \State $从src非阻塞接收Mloc, countloc$
          \EndFor
          
          \For{m=0:M\%(mpiWorldSize-1)}
          \State $sumCount \gets sumCount + MONO_2(M, Nloc)$
          \EndFor 
          
          \For{i=1:mpiWorldSize}
          \State 等待任何未达的消息Mloc, countloc
          \State $sumM \gets sumM + Mloc$
          \State $sumCount \gets sumCount + countloc$
          \EndFor
          
          \If{sumCount/sumM > Prob}
          \State $lastvalid \gets Nloc$
          \State $Nup \gets Nloc$
          \State $Nnew \gets \frac{Nup+Ndown}{2}$
          \Else
          \State $Ndown \gets Nloc$
          \State $Nnew \gets \frac{Nup+Ndown}{2}$
          \EndIf
          \If{$abs(Nup - Ndown) < \lambda$}
          \State $stop = 1$
          \EndIf
          \State $Nloc \gets Nnew$
          \State $非阻塞发送Nloc和stop给其他worker$
          \EndWhile
          \If{lastvalid != -1}
          \State $n \gets lastvalid$
          \State return $n$
          \Else
          \State 未搜索到合适的N值
          \EndIf
          \algstore{breakagain}
        \end{algorithmic}
\end{algorithm}
          
\begin{algorithm}
  \caption{基于MPI的多机并行算法，worker部分}
  \label{alg:mpi}
  \begin{algorithmic}[1]
    \algrestore{breakagain}
    \Else 
    \State $Nup \gets N_0$
    \State $Ndown \gets 0$
    \State $Nloc \gets \frac{Nup+Ndown}{2}$
    \State $countloc \gets 0$
    \While{$true$}
    
    \For{m=0:(int)M/(mpiWorldSize-1)}
    \State $countloc \gets countloc + MONO_2(M, Nloc)$
    \EndFor 
    
    \State 非阻塞发送(int)M/(mpiWorldSize-1), countloc给boss
    \State $countloc \gets 0$
    
    \For{m=0:(int)M/(mpiWorldSize-1)}
    \State $countloc \gets countloc + MONO_2(M, (Nloc+Ndown)/2)$
    \If{m\%10=0}
    \State 检测是否收到boss的消息，如果是则$break$
    \EndIf
    \EndFor
    
    \State 阻塞接受来自boss的消息Nnew, stop
    
    \If{$stop=1$}
    \State $break$
    \Else
    \State $对比Nnew和(Nloc+Ndown)/2，相应地更新Mloc, Nloc$
    \EndIf

    \EndWhile
    \EndIf
    \EndProcedure
	\end{algorithmic}
\end{algorithm}

